{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1929e1-3fac-4f5b-b397-182fd0cc728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: bleach in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2025.10.5)\n",
      "Requirement already satisfied: charset-normalizer in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (3.11)\n",
      "Requirement already satisfied: protobuf in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (6.33.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: requests in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: webencodings in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (0.5.1)\n",
      "Using cached kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.7.4.5 python-slugify-8.0.4 text-unidecode-1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436c992e-9cbb-4006-bffe-c181be109281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.path.expanduser('~/Desktop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b376e9-1c38-4d29-b802-92efa8877a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/yusra/Desktop/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/antoinebourgois2/wikipedia-ai-glossary\n",
      "License(s): CC0-1.0\n",
      "wikipedia-ai-glossary.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d antoinebourgois2/wikipedia-ai-glossary -p ./data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d40afd8-1889-4f9d-8a3e-e5621cbcf1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('./data/wikipedia-ai-glossary.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ba8184-3f95-4842-9ad5-5def8227d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wikipedia_AI_Glossary.csv', 'wikipedia-ai-glossary.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('./data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52da9caa-5fc3-4294-98fd-b7700d2d7441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Link', 'Title', 'Wikipedia_page_description',\n",
      "       'High_dimensional_embeddings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/Wikipedia_AI_Glossary.csv')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7ce2cbd-5319-472a-9656-5455b64875e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b3341fb-2686-4c73-8f9f-6731107a2438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset boyutu: 343\n",
      "Örnek terim: abductive logic programming\n",
      "Örnek açıklama: Logic programming using abductive reasoningAbductive logic programming ALP is a high-level knowledge-representation framework that can be used to solve problems declaratively, based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates. Problem solving is effected by deriving hypotheses on these abducible predicates abductive hypotheses as solutions of problems to be solved. These problems can be either observations that need to be explained as in classical abduction or goals to be achieved as in normal logic programming. It can be used to solve problems in diagnosis, planning, natural language and machine learning. It has also been used to interpret negation as failure as a form of abductive reasoning.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 2️⃣ Dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv('./data/Wikipedia_AI_Glossary.csv')\n",
    "df = df.dropna(subset=[\"Title\", \"Wikipedia_page_description\"])\n",
    "df[\"Wikipedia_page_description\"] = df[\"Wikipedia_page_description\"].apply(lambda x: x.replace(\"\\n\", \" \").strip())\n",
    "texts = df[\"Wikipedia_page_description\"].tolist()\n",
    "\n",
    "print(f\"Dataset boyutu: {len(df)}\")\n",
    "print(f\"Örnek terim: {df['Title'].iloc[0]}\")\n",
    "print(f\"Örnek açıklama: {texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec7176d9-7df9-4277-a3c5-0088508862f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 2/2 [00:00<00:00,  7.19it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size):\n\u001b[32m      8\u001b[39m     batch = texts[i:i+batch_size]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     batch_embeddings = \u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     embeddings.extend(batch_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag_env/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1152\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1150\u001b[39m             all_embeddings = np.asarray([emb.float().numpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[32m   1151\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1152\u001b[39m             all_embeddings = np.asarray([\u001b[43memb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[32m   1153\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np.ndarray):\n\u001b[32m   1154\u001b[39m     all_embeddings = [torch.from_numpy(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m all_embeddings]\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3️⃣ Embeddings (batch ile)\n",
    "# -------------------------------\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = []\n",
    "batch_size = 50\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    batch_embeddings = embedder.encode(batch, show_progress_bar=True)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "096eb5ef-9c42-46c5-9de1-c201e0ca62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4️⃣ Chroma Vector Store\n",
    "# -------------------------------\n",
    "client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "if \"ai_glossary\" in [c.name for c in client.list_collections()]:\n",
    "    client.delete_collection(\"ai_glossary\")\n",
    "\n",
    "collection = client.create_collection(\"ai_glossary\")\n",
    "for i, (text, emb) in enumerate(zip(texts, embeddings)):\n",
    "    collection.add(documents=[text], embeddings=[emb.tolist()], ids=[str(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb35986b-2b1b-4073-8d6a-15e075ca9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5️⃣ QA Model (Tiny Flan-T5)\n",
    "# -------------------------------\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", tokenizer=\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8de129ff-928d-42fd-8bfc-a9be001534e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6️⃣ RAG Query Fonksiyonu\n",
    "# -------------------------------\n",
    "def rag_query(question, top_k=3):\n",
    "    query_vec = embedder.encode([question])\n",
    "    results = collection.query(query_embeddings=query_vec.tolist(), n_results=top_k)\n",
    "    top_docs = results[\"documents\"][0]\n",
    "\n",
    "    context = \" \".join(top_docs)\n",
    "    context = context[:3000]  # Uzun context için limit\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI expert. Answer the question using the following context.\n",
    "Provide a detailed explanation in 3-5 sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    answer = qa_model(prompt, max_new_tokens=600, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    return answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d04c8d-0f64-457a-a5fb-0bd912c13b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
