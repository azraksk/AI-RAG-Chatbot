{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1929e1-3fac-4f5b-b397-182fd0cc728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: bleach in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2025.10.5)\n",
      "Requirement already satisfied: charset-normalizer in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (3.11)\n",
      "Requirement already satisfied: protobuf in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (6.33.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: requests in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: webencodings in /Users/yusra/rag_env/lib/python3.12/site-packages (from kaggle) (0.5.1)\n",
      "Using cached kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.7.4.5 python-slugify-8.0.4 text-unidecode-1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436c992e-9cbb-4006-bffe-c181be109281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.path.expanduser('~/Desktop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b376e9-1c38-4d29-b802-92efa8877a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/yusra/Desktop/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/antoinebourgois2/wikipedia-ai-glossary\n",
      "License(s): CC0-1.0\n",
      "wikipedia-ai-glossary.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d antoinebourgois2/wikipedia-ai-glossary -p ./data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d40afd8-1889-4f9d-8a3e-e5621cbcf1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('./data/wikipedia-ai-glossary.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ba8184-3f95-4842-9ad5-5def8227d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wikipedia_AI_Glossary.csv', 'wikipedia-ai-glossary.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('./data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52da9caa-5fc3-4294-98fd-b7700d2d7441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Link', 'Title', 'Wikipedia_page_description',\n",
      "       'High_dimensional_embeddings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/Wikipedia_AI_Glossary.csv')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ce2cbd-5319-472a-9656-5455b64875e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3341fb-2686-4c73-8f9f-6731107a2438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset boyutu: 343\n",
      "Örnek terim: abductive logic programming\n",
      "Örnek açıklama: Logic programming using abductive reasoningAbductive logic programming ALP is a high-level knowledge-representation framework that can be used to solve problems declaratively, based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates. Problem solving is effected by deriving hypotheses on these abducible predicates abductive hypotheses as solutions of problems to be solved. These problems can be either observations that need to be explained as in classical abduction or goals to be achieved as in normal logic programming. It can be used to solve problems in diagnosis, planning, natural language and machine learning. It has also been used to interpret negation as failure as a form of abductive reasoning.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 2️⃣ Dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv('./data/Wikipedia_AI_Glossary.csv')\n",
    "df = df.dropna(subset=[\"Title\", \"Wikipedia_page_description\"])\n",
    "df[\"Wikipedia_page_description\"] = df[\"Wikipedia_page_description\"].apply(lambda x: x.replace(\"\\n\", \" \").strip())\n",
    "texts = df[\"Wikipedia_page_description\"].tolist()\n",
    "\n",
    "print(f\"Dataset boyutu: {len(df)}\")\n",
    "print(f\"Örnek terim: {df['Title'].iloc[0]}\")\n",
    "print(f\"Örnek açıklama: {texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7176d9-7df9-4277-a3c5-0088508862f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d777146bc1254b4e96b6cbfadd6539fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357bfb492f914a5e98723f8bb3652a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02221bd93f04fdfa35b5ed5ee534720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c48801a7f9474fa9042f92fbe404fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d2938f52ff46ecb3c5beef92259290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc5bf13737146f997b0a76483195337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f78a989c7540fb9b01acbf3d8f1cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 3️⃣ Embeddings (batch ile)\n",
    "# -------------------------------\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = []\n",
    "batch_size = 50\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    batch_embeddings = embedder.encode(batch, show_progress_bar=True)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "096eb5ef-9c42-46c5-9de1-c201e0ca62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4️⃣ Chroma Vector Store\n",
    "# -------------------------------\n",
    "client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "if \"ai_glossary\" in [c.name for c in client.list_collections()]:\n",
    "    client.delete_collection(\"ai_glossary\")\n",
    "\n",
    "collection = client.create_collection(\"ai_glossary\")\n",
    "for i, (text, emb) in enumerate(zip(texts, embeddings)):\n",
    "    collection.add(documents=[text], embeddings=[emb.tolist()], ids=[str(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb35986b-2b1b-4073-8d6a-15e075ca9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5️⃣ QA Model (Tiny Flan-T5)\n",
    "# -------------------------------\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", tokenizer=\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de129ff-928d-42fd-8bfc-a9be001534e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6️⃣ RAG Query Fonksiyonu\n",
    "# -------------------------------\n",
    "def rag_query(question, top_k=3):\n",
    "    query_vec = embedder.encode([question])\n",
    "    results = collection.query(query_embeddings=query_vec.tolist(), n_results=top_k)\n",
    "    top_docs = results[\"documents\"][0]\n",
    "\n",
    "    context = \" \".join(top_docs)\n",
    "    context = context[:3000]  # Uzun context için limit\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI expert. Answer the question using the following context.\n",
    "Provide a detailed explanation in 3-5 sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    answer = qa_model(prompt, max_new_tokens=600, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    return answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d04c8d-0f64-457a-a5fb-0bd912c13b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is NLP?\n",
      "Answer: An interdisciplinary subfield of computer science and linguistics.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 7️⃣ Test Soru\n",
    "# -------------------------------\n",
    "question = \"What is NLP?\"\n",
    "answer = rag_query(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b3de-8d3b-4043-a012-576cc1c256b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b8030-efd6-41f2-a2dd-ee8c30f4fedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
